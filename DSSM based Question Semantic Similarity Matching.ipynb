{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于DSSM的问题语义相似度匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSSM的全称是Deep Structured Semantic Model或者Deep Semantic Similarity Model。\n",
    "DSSM由微软研究院深度学习研究中心开发，是一个利用深度神经网络把文本（句子，queries，实体等）表示成向量，并且计算文本相似度的模型和方法。\n",
    "DSSM在信息检索和网络文本排序中有广泛的应用([Huang et al. 2013](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/); [Shen et al. 2014a](https://www.microsoft.com/en-us/research/publication/learning-semantic-representations-using-convolutional-neural-networks-for-web-search/),[2014b](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/); [Palangi et al. 2016](https://www.microsoft.com/en-us/research/publication/deep-sentence-embedding-using-long-short-term-memory-networks-analysis-application-information-retrieval/)), 广告相关性, 实体搜索和有趣性任务([Gao et al. 2014a](https://www.microsoft.com/en-us/research/publication/modeling-interestingness-with-deep-neural-networks/), 问答([Yih et al., 2014](https://www.microsoft.com/en-us/research/publication/semantic-parsing-for-single-relation-question-answering/)), 图片描述([Fang et al., 2014](https://arxiv.org/abs/1411.4952)), 以及机器翻译 ([Gao et al., 2014b](https://www.microsoft.com/en-us/research/publication/learning-continuous-phrase-representations-for-translation-modeling/)) etc. \n",
    "\n",
    "\n",
    "DSSM可以被用作开发latent semantic models，把不同的实体投影到同一个低维度的语义空间，然后用于文本分类，排序等任务。举例来说，在网络搜索任务中，文本和搜索短语的相关性可以用vector之间的距离表示。[He et al., 2014](https://www.microsoft.com/en-us/research/publication/deep-learning-for-natural-language-processing-theory-and-practice-tutorial/).\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "给定一对文本，例如搜索一个关键词和一组网络文本，模型会把他们分别转化成低维的连续向量，然后用cosine相似度来计算文本的相似性。\n",
    "\n",
    "![](http://kubicode.me/img/Study-With-Deep-Structured-Semantic-Model/dssm_arch.png)\n",
    "\n",
    "从上图中我们看到，给定一个query($Q$)和一组文档($D_1, D_2, \\ldots, D_n$)，模型可以生成一组隐向量表示(semantic features)，然后这些semantic features就可以用来计算文本相似度，最终用于文本排序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图中我们看到，query和document都被编码成了向量。\n",
    "虽然[bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model)是人们常用的文本表示方式，但是它丢失了文本中单词之间的位置关系信息。\n",
    "卷积或者循环神经网络，由于它们编码单词位置信息的能力，在很多NLP问题上有更好的表现。在这份材料中，我们会使用LSTM模型来编码term vector [Palangi et. al.](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf)。\n",
    "\n",
    "我们使用一个比较小的问答数据集来训练这个模型。这份notebook的作用是展示如何构建一个DSSM模型，而不是为了用它达到State-of-the-art的表现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "\n",
    "# import cntk as C\n",
    "# import cntk.tests.test_utils\n",
    "# cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "# C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "我们使用一组问答数据集来展示如何使用DSSM模型。\n",
    "这组数据集包含很多对[问答](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf)句子。\n",
    "我们把这些数据预处理成两个部分：\n",
    "- 词汇文件：问题和回答各有一个单词文件。问题和答案分别有1204和1019个单词。\n",
    "- 问答句子：包含一个训练集和一个验证集。这些文本都被做成了[CTF格式](https://cntk.ai/pythondocs/CNTK_202_Language_Understanding.html)。训练集有3500对句子，验证集有409对。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: train.pair.tok.ctf\n",
      "http://www.cntk.ai/jup/dat/DSSM/train.pair.tok.ctf.csv\n",
      "Download completed\n",
      "Starting download: valid.pair.tok.ctf\n",
      "http://www.cntk.ai/jup/dat/DSSM/valid.pair.tok.ctf.csv\n",
      "Download completed\n",
      "Starting download: vocab_Q.wl\n",
      "http://www.cntk.ai/jup/dat/DSSM/vocab_Q.wl.csv\n",
      "Download completed\n",
      "Starting download: vocab_A.wl\n",
      "http://www.cntk.ai/jup/dat/DSSM/vocab_A.wl.csv\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "location = os.path.normpath('data/DSSM')\n",
    "data = {\n",
    "  'train': { 'file': 'train.pair.tok.ctf' },\n",
    "  'val':{ 'file': 'valid.pair.tok.ctf' },\n",
    "  'query': { 'file': 'vocab_Q.wl' },\n",
    "  'answer': { 'file': 'vocab_A.wl' }\n",
    "}\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "if not os.path.exists(location):\n",
    "    os.mkdir(location)\n",
    "     \n",
    "for item in data.values():\n",
    "    path = os.path.normpath(os.path.join(location, item['file']))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", path)\n",
    "        \n",
    "    else:\n",
    "        print(\"Starting download:\", item['file'])\n",
    "        url = \"http://www.cntk.ai/jup/dat/DSSM/%s.csv\"%(item['file'])\n",
    "        print(url)\n",
    "        download(url, path)\n",
    "        print(\"Download completed\")\n",
    "    item['file'] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取\n",
    "\n",
    "我们用CTF deserializer来读取数据。当然，你也可以选择用别的方法自己预处理数据。这里提供的CTF reader也提供打乱样本顺序的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary size (QRY-stands for question and ANS stands for answer)\n",
    "QRY_SIZE = 1204\n",
    "ANS_SIZE = 1019\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query = C.io.StreamDef(field='S0', shape=QRY_SIZE,  is_sparse=True),\n",
    "         answer  = C.io.StreamDef(field='S1', shape=ANS_SIZE, is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\DSSM\\train.pair.tok.ctf\n",
      "data\\DSSM\\valid.pair.tok.ctf\n"
     ]
    }
   ],
   "source": [
    "train_file = data['train']['file']\n",
    "print(train_file)\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    train_source = create_reader(train_file, is_training=True)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(train_file, os.getcwd()))\n",
    "\n",
    "validation_file = data['val']['file']\n",
    "print(validation_file)\n",
    "if os.path.exists(validation_file):\n",
    "    val_source = create_reader(validation_file, is_training=False)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(validation_file, os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "LSTM-RNN模型可以按照顺序读入句子中的单词，抽取单词中的信息，然后embed成一个vector。\n",
    "在DSSM模型中，我们采用句子的最后一个hidden state来作为整个句子的vector表示。\n",
    "这个vector通过两次Feedforward神经网络就可以作为query vector。\n",
    "\n",
    "\n",
    "\n",
    "                                                                        \"query vector\"\n",
    "                                                                              ^\n",
    "                                                                              |\n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                         +---------+  \n",
    "                                                                         | Dropout |  \n",
    "                                                                         +---------+\n",
    "                                                                              ^\n",
    "                                                                              |         \n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                          +------+   \n",
    "                                                                          | last |  \n",
    "                                                                          +------+  \n",
    "                                                                              ^  \n",
    "                                                                              |         \n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |\n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                              | Embed |  | Embed |  | Embed |  | Embed |  | Embed | \n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                    query  ------>+--------->+--------->+--------->+--------->+\n",
    "    \n",
    " \n",
    "类似地，我们可以把答案句子编码成answer vector。我们首先定义模型的输入，分别是query和answer的sequence，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "qry = C.sequence.input_variable(QRY_SIZE)\n",
    "ans = C.sequence.input_variable(ANS_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个CNTK的sequence都包含一个dynamic axis，表示sequence的长度。\n",
    "直观来说，当你的sequence有不同的长度和不同的单词表大小，他们都应该有一个dynamic axis。\n",
    "这时候就需要声明named axis。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "axis_qry = C.Axis.new_unique_dynamic_axis('axis_qry')\n",
    "qry = C.sequence.input_variable(QRY_SIZE, sequence_axis=axis_qry)\n",
    "\n",
    "axis_ans = C.Axis.new_unique_dynamic_axis('axis_ans')\n",
    "ans = C.sequence.input_variable(ANS_SIZE, sequence_axis=axis_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在创建模型之前我们先定义一些模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMB_DIM   = 25 # Embedding dimension\n",
    "HIDDEN_DIM = 50 # LSTM dimension\n",
    "DSSM_DIM = 25 # Dense layer dimension  \n",
    "NEGATIVE_SAMPLES = 5\n",
    "DROPOUT_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(qry, ans):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        qry_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='q_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout qdo1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='q_enc')\n",
    "        ])\n",
    "        \n",
    "        ans_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='a_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout ado1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='a_enc')\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        'query_vector': qry_vector(qry),\n",
    "        'answer_vector': ans_vector(ans)\n",
    "    }\n",
    "\n",
    "# Create the model and store reference in `network` dictionary\n",
    "network = create_model(qry, ans)\n",
    "\n",
    "network['query'], network['axis_qry'] = qry, axis_qry\n",
    "network['answer'], network['axis_ans'] = ans, axis_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "现在我们已经创建了模型，下一步就是找到一个合适的损失函数。这个损失函数的功能是，如果我们的问题和一个正确的答案匹配在一起，这个损失就应该是一个接近0的很小的数字，如果问题和答案不匹配，那么损失函数应该给我们返回一个接近1的数字。换句话说，这个损失函数最大化问题和正确答案之间的相似度，最小化问题与错误答案之间的相似度。\n",
    "\n",
    "DSSM经常被用在信息检索类问题中。往往给定一个搜索的短语或问题，我们需要在海量的文本中寻找正确答案。输入的数据是一个问题和一个潜在的答案（文本或者广告），这些文本或者广告可能会被点击。我们的目标是要提高被点击的概率，也就是说被搜索到的文档或广告与搜索关键词比较相关。一种做法是训练一个分类器，这个分类器可以预测链接是否被点开。为了训练这样一个模型，我们需要被点开的搜索短语和链接，也需要没有被点开的链接。一种模拟没有被点开的链接的方法是从当前minibatch中随机采样其他query产生的链接。这就是 `cosine_distance_with_negative_samples` 这个function在做的事情。注意，这个function的返回值1表示正确的问题与答案，0表示错误的问题与答案，我们把它叫做*similarity*。所以，我们用1-`cosine_distance_with_negative_samples`作为损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_loss(vector_a, vector_b):\n",
    "    qry_ans_similarity = C.cosine_distance_with_negative_samples(vector_a, \\\n",
    "                                                                 vector_b, \\\n",
    "                                                                 shift=1, \\\n",
    "                                                                 num_negative_samples=5)\n",
    "    return 1 - qry_ans_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_EPOCHS = 5\n",
    "EPOCH_SIZE = 10000\n",
    "MINIBATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "def create_trainer(reader, network):\n",
    "    \n",
    "    # Setup the progress updater\n",
    "    progress_writer = C.logging.ProgressPrinter(tag='Training', num_epochs=MAX_EPOCHS)\n",
    "\n",
    "    # Set learning parameters\n",
    "    lr_per_sample     = [0.0015625]*20 + \\\n",
    "                        [0.00046875]*20 + \\\n",
    "                        [0.00015625]*20 + \\\n",
    "                        [0.000046875]*10 + \\\n",
    "                        [0.000015625]\n",
    "    lr_schedule       = C.learning_parameter_schedule_per_sample(lr_per_sample, \\\n",
    "                                                 epoch_size=EPOCH_SIZE)\n",
    "    mms               = [0]*20 + [0.9200444146293233]*20 + [0.9591894571091382]\n",
    "    mm_schedule       = C.learners.momentum_schedule(mms, \\\n",
    "                                                     epoch_size=EPOCH_SIZE, \\\n",
    "                                                     minibatch_size=MINIBATCH_SIZE)\n",
    "    l2_reg_weight     = 0.0002\n",
    "\n",
    "    model = C.combine(network['query_vector'], network['answer_vector'])\n",
    "\n",
    "    #Notify the network that the two dynamic axes are indeed same\n",
    "    query_reconciled = C.reconcile_dynamic_axes(network['query_vector'], network['answer_vector'])\n",
    "  \n",
    "    network['loss'] = create_loss(query_reconciled, network['answer_vector'])\n",
    "    network['error'] = None\n",
    "\n",
    "    print('Using momentum sgd with no l2')\n",
    "    dssm_learner = C.learners.momentum_sgd(model.parameters, lr_schedule, mm_schedule)\n",
    "\n",
    "    network['learner'] = dssm_learner\n",
    " \n",
    "    print('Using local learner')\n",
    "    # Create trainer\n",
    "    return C.Trainer(model, (network['loss'], network['error']), network['learner'], progress_writer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using momentum sgd with no l2\n",
      "Using local learner\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the trainer\n",
    "trainer = create_trainer(train_source, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train \n",
    "def do_train(network, trainer, train_source):\n",
    "    # define mapping from intput streams to network inputs\n",
    "    input_map = {\n",
    "        network['query']: train_source.streams.query,\n",
    "        network['answer']: train_source.streams.answer\n",
    "        } \n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(MAX_EPOCHS):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * EPOCH_SIZE\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_source.next_minibatch(MINIBATCH_SIZE, input_map= input_map)  # fetch minibatch\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += MINIBATCH_SIZE\n",
    "\n",
    "        trainer.summarize_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per 1 samples: 0.0015625\n",
      "Momentum per 1 samples: 0.0\n",
      "Finished Epoch[1 of 5]: [Training] loss = 0.343046 * 1522, metric = 0.00% * 1522 5.720s (266.1 samples/s);\n",
      "Finished Epoch[2 of 5]: [Training] loss = 0.102804 * 1530, metric = 0.00% * 1530 3.464s (441.7 samples/s);\n",
      "Finished Epoch[3 of 5]: [Training] loss = 0.066461 * 1525, metric = 0.00% * 1525 3.402s (448.3 samples/s);\n",
      "Finished Epoch[4 of 5]: [Training] loss = 0.048511 * 1534, metric = 0.00% * 1534 3.390s (452.5 samples/s);\n",
      "Finished Epoch[5 of 5]: [Training] loss = 0.035384 * 1510, metric = 0.00% * 1510 3.383s (446.3 samples/s);\n"
     ]
    }
   ],
   "source": [
    "do_train(network, trainer, train_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n",
    "\n",
    "当我们训练完模型后，我们需要选择一个训练与验证错误率相近的模型。\n",
    "可以通过选择不同的epoch数量来选择更好的模型。\n",
    "通过这种方式选择的模型最终被用于预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validate\n",
    "def do_validate(network, val_source):\n",
    "    # process minibatches and perform evaluation\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n",
    "\n",
    "    val_map = {\n",
    "        network['query']: val_source.streams.query,\n",
    "        network['answer']: val_source.streams.answer\n",
    "        } \n",
    "\n",
    "    evaluator = C.eval.Evaluator(network['loss'], progress_printer)\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 100\n",
    "        data = val_source.next_minibatch(minibatch_size, input_map=val_map)\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "\n",
    "        evaluator.test_minibatch(data)\n",
    "\n",
    "    evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-35]: metric = 0.02% * 410;\n"
     ]
    }
   ],
   "source": [
    "do_validate(network, val_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 预测\n",
    "\n",
    "我们会把query和answer都转化成vector。然后计算它们之间的cosine similarity。这些cosine similarity的分数可以用来对搜索的网页排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Indices: [1202, 1154, 267, 321, 357, 648, 1070, 905, 549, 6, 1203]\n",
      "Answer Indices: [1017, 135, 91, 137, 1018]\n",
      "Poor Answer Indices: [1017, 501, 452, 533, 1018]\n"
     ]
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "answers_wl = [line.rstrip('\\n') for line in open(data['answer']['file'])]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "answers_dict = {answers_wl[i]:i for i in range(len(answers_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "qry = 'BOS what contribution did  e1  made to science in 1665 EOS'\n",
    "ans = 'BOS book author book_editions_published EOS'\n",
    "ans_poor = 'BOS language human_language main_country EOS'\n",
    "\n",
    "qry_idx = [query_dict[w+' '] for w in qry.split()] # convert to query word indices\n",
    "print('Query Indices:', qry_idx)\n",
    "\n",
    "ans_idx = [answers_dict[w+' '] for w in ans.split()] # convert to answer word indices\n",
    "print('Answer Indices:', ans_idx)\n",
    "\n",
    "ans_poor_idx = [answers_dict[w+' '] for w in ans_poor.split()] # convert to fake answer word indices\n",
    "print('Poor Answer Indices:', ans_poor_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the one hot representations\n",
    "qry_onehot = np.zeros([len(qry_idx),len(query_dict)], np.float32)\n",
    "for t in range(len(qry_idx)):\n",
    "    qry_onehot[t,qry_idx[t]] = 1\n",
    "    \n",
    "ans_onehot = np.zeros([len(ans_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_idx)):\n",
    "    ans_onehot[t,ans_idx[t]] = 1\n",
    "    \n",
    "ans_poor_onehot = np.zeros([len(ans_poor_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_poor_idx)):\n",
    "    ans_poor_onehot[t, ans_poor_idx[t]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query to Answer similarity: 0.99995367043\n",
      "Query to poor-answer similarity: 0.999941420215\n"
     ]
    }
   ],
   "source": [
    "qry_embedding = network['query_vector'].eval([qry_onehot])\n",
    "ans_embedding = network['answer_vector'].eval([ans_onehot])\n",
    "ans_poor_embedding = network['answer_vector'].eval([ans_poor_onehot])\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print('Query to Answer similarity:', 1-cosine(qry_embedding, ans_embedding))\n",
    "print('Query to poor-answer similarity:', 1-cosine(qry_embedding, ans_poor_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
